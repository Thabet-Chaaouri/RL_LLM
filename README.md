# RL_LLM

Traditional method:
- Use an auxiliary reward model and fine-tune the model of interest so that it maximizes this given reward via the machinery of RL and it generates high-reward samples more often and low-reward samples less often
