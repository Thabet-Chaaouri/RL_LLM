# RL_LLM

## The SFT + RL Process


## The PPO method

Traditional method:
- Use an auxiliary reward model and fine-tune the model of interest so that it maximizes this given reward via the machinery of RL and it generates high-reward samples more often and low-reward samples less often


## The DPO method
[article](https://huggingface.co/blog/dpo-trl)
