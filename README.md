# RLHF_LLM

## The SFT + RL Process

Core to starting the RLHF process is having a model that responds well to diverse instructions.

#### Supervised Fine-tuning

#### Reward model training
The underlying goal is to get a model or system that takes in a sequence of text, and returns a scalar reward which should numerically represent the human preference. 
The system can be an end-to-end LM, or a modular system outputting a reward. An intuition would be that these preference models need to have similar capacity to understand the text given to them as a model would need in order to generate said text.

The training dataset for RM is generated by sampling a set of prompts, then using the instruction tuned LLM (with may be other LLMs) to generate multiple outputs.
Human annotators are used to rank the generated text outputs from the LM. ranking has benn proven to be a better choice to define preference than giving scalar values. Then, the ranking is normalized into a scalar reward signal for training.

At this point in the RLHF system, we have a preference model that takes in any text and assigns it a score of how well humans perceive it.

#### Fine-tuning with RL

Fine-tuning some or all of the parameters of a copy of the initial LM with a policy-gradient RL algorithm.
PPO has been around for a relatively long time. The relative maturity of this method made it a favorable choice for scaling up to the new application of distributed training for RLHF.

## The PPO method

Traditional method:
- Use an auxiliary reward model and fine-tune the model of interest so that it maximizes this given reward via the machinery of RL and it generates high-reward samples more often and low-reward samples less often

How it works?
- The policy is a language model that takes in a prompt and returns a sequence of text
- The action space of this policy is all the tokens corresponding to the vocabulary of the language model (often on the order of 50k tokens)
- The observation space is the distribution of possible input token sequences
- The reward function is a combination of the preference model and a constraint on policy shift

Given a prompt x, the text y is generated. (x,y) are passed to the RM, which returns a scalar notion of “preferability” rθ.

In addition, per-token probability distributions from the RL policy are compared to the ones from the initial model to compute a penalty on the difference between them. this penalty has been designed as a scaled version of the Kullback–Leibler (KL) divergence between these sequences of distributions over tokens rKL.

The final reward sent to the RL update rule is r=rθ−λrKL.



​


## The DPO method
[HF Blog article](https://huggingface.co/blog/dpo-trl)
